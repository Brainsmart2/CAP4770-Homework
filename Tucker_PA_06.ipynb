#1. k-means using scikit-learn

# Import needed packages
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

healthy = pd.read_csv('healthy_lifestyle.csv')

# Input the number of clusters
number = int(input())

# Define input features
X = healthy[['sunshine_hours', 'happiness_levels']]

# Use StandardScaler() to standardize input features
scaler = StandardScaler()
X = scaler.fit_transform(X)
X = pd.DataFrame(X, columns=['sunshine_hours', 'happiness_levels'])
X = X.dropna()

# Initialize a k-means clustering algorithm with a user-defined number of clusters, init='random', n_init=10, 
# random_state=123, and algorithm='elkan'
var1 = KMeans(n_clusters=number, init='random', n_init=10, random_state=123, algorithm='elkan')

# Fit the algorithm to the input features
var1.fit(X)

# Find and print the cluster centroids
centroid = var1.cluster_centers_
print("Centroids:", np.round(centroid,4))

# Find and print the cluster inertia
inertia = var1.inertia_
print("Inertia:", np.round(inertia,4))

#2. Hierarchial clustering using scikit-learn

import pandas as pd
import seaborn as sns
import numpy as np

# Import needed sklearn packages
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler

# Import needed scipy packages
from scipy.spatial.distance import pdist
from scipy.spatial.distance import squareform
from scipy.cluster.hierarchy import linkage

# Silence warning
import warnings
warnings.filterwarnings('ignore')


healthy = pd.read_csv('healthy_lifestyle.csv')

# Input the number of clusters
number = int(input())

# Define input features
X = healthy[['sunshine_hours', 'happiness_levels']]

# Use StandardScaler() to standardize input features
scaler = StandardScaler()
X = scaler.fit_transform(X)
X = pd.DataFrame(X, columns=['sunshine_hours', 'happiness_levels'])
X = X.dropna()

# Initialize and fit an agglomerative clustering model using ward linkage in scikit-learn, with a user-defined
# number of clusters
ACM = AgglomerativeClustering(n_clusters=number, linkage='ward')

# Add cluster labels to input feature dataframe
X['labels']= ACM.fit_predict(X)
print(X.head())

# Perform agglomerative clustering using SciPy

# Calculate the distances between all instances
d_matrix = pdist(X[['sunshine_hours', 'happiness_levels']])

# Convert the distance matrix to a square matrix
s_d_matrix = squareform(d_matrix)

# Define a clustering model with ward linkage
clustersHealthyScipy = linkage(d_matrix, method='ward')

print('First five rows of the linkage matrix from SciPy:\n', np.round(clustersHealthyScipy[:5, :], 0))

#3. DBSCAN using scikit-learn

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN

# Load the full grocery customer dataset and take a random sample of 500 instances
data = pd.read_csv('customer_personality.csv').sample(500, random_state=123)

# Use StandardScaler() to standardize input features
X = data[['Fruits', 'Meats']]
scaler = StandardScaler()
X = scaler.fit_transform(X)
X = pd.DataFrame(X)

# Apply DBSCAN with epsilon=1 and min_samples = 8   
dbscan = DBSCAN(eps=1, min_samples=8)
dbscan = dbscan.fit(X)

# Print the cluster labels and core point indices
print('Labels:', dbscan.labels_)
print('Core points:', dbscan.core_sample_indices_) 
print('Number of core points:', np.sum(dbscan.core_sample_indices_ >= 0))

num_outliers = np.sum(dbscan.labels_ == -1)
print("Number of Outliers: ", num_outliers)

# Add the cluster labels to the dataset as strings
data['clusters'] = dbscan.labels_.astype(str)

# Sort by cluster label (for plotting purposes)
data.sort_values(by='clusters', inplace=True)

# Plot clusters on the original data
p = sns.scatterplot(data=data, x='Fruits',
                    y='Meats', hue='clusters',
                    style='clusters')
p.set_xlabel('Fruits', fontsize=16)
p.set_ylabel('Meats', fontsize=16)
p.legend(title='DBSCAN')
