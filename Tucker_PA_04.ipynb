## 1. Linear Regression using scikit-learn

# Import needed packages for regression
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Silence warning from sklearn
import warnings
warnings.filterwarnings('ignore')

# Input feature values for a sample instance
carat = float(input())
table = float(input())

diamonds = pd.read_csv('diamonds.csv')

# Define input and output features
X = diamonds[['carat', 'table']]
y = diamonds['price']

# Initialize a multiple linear regression model
simpLinModel = LinearRegression()

# Fit the multiple linear regression model to the input and output features
simpLinModel.fit(X,y)

# Get estimated intercept weight
intercept = simpLinModel.intercept_
print('Intercept is', round(intercept, 3))

# Get estimated weights for carat and table features
coefficients = simpLinModel.coef_
print('Weights for carat and table features are', np.round(coefficients, 3))

# Predict the price of a diamond with the user-input carat and table values
prediction = simpLinModel.predict([[carat, table]]) #fix prediction -> Need to input Carat & Table vals
print('Predicted price is', np.round(prediction, 2))

## 2. Logistic Regression using scikit-learn

# Import the necessary libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
import seaborn as sns

# Load nbaallelo_log.csv into a dataframe
NBA = pd.read_csv('nbaallelo_log.csv')

# Create binary feature for game_result with 0 for L and 1 for W
NBA['win'] = NBA['game_result'].replace(to_replace = ['L', 'W'], 
                                        value = [int(0), int(1)])

# Store relevant columns as variables
X = NBA[['elo_i']]
y = NBA[['win']]

# Initialize and fit the logistic model using the LogisticRegression() function
LogisticModel = LogisticRegression(penalty=None)
LogisticModel.fit(X, np.ravel(y))

# Print the weights for the fitted model
print('w1:', LogisticModel.coef_)

# Print the intercept of the fitted model
print('w0:', LogisticModel.intercept_)

# Find the proportion of instances correctly classified
score = LogisticModel.score(X,y)
print(round(score, 3))

## 3. Support Vector Classifier using scikit-learn

# Import the necessary packages
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

heart = pd.read_csv('heart.csv')

# Input features: thalach and age
X = heart[['thalach', 'age']]

# Output feature: target
y = heart[['target']]

# Create training and testing data with 75% training data and 25% testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)

# Scale the input features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize a support vector classifier with C=0.2 and a maximum of 500 iterations
SVC = LinearSVC(C=0.2, max_iter=500, random_state=123)
# Fit the support vector classifier according to the training data
SVC.fit(X_train, np.ravel(y_train))
# Evaluate model on testing data
score = SVC.score(X_test, np.ravel(y_test))
print(np.round(score, 3))

# Print the model weights
# w0
print('w0:', np.round(SVC.intercept_, 3))
# w1 and w2
print('w1 and w2:', np.round(SVC.coef_, 3))

## 4. k-Nearest Neighbors using scikit-learn 

# Import needed packages for classification
# Import packages for evaluation
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
skySurvey = pd.read_csv('SDSS.csv')

# Create a new feature from u - g
skySurvey['u_g'] = skySurvey['u'] - skySurvey['g']

# Create dataframe X with features redshift and u_g
X = skySurvey[['redshift', 'u_g']]

# Create dataframe y with feature class
y = skySurvey[['class']]

np.random.seed(42)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Initialize model with k=3
skySurveyKnn = KNeighborsClassifier(n_neighbors=3)

# Fit model using X_train and y_train
skySurveyKnn.fit(X_train, y_train)

# Find the predicted classes for X_test
y_pred = skySurveyKnn.predict(X_test)

# Calculate accuracy score
score = accuracy_score(y_test, y_pred)

# Print accuracy score
print('Accuracy score is ', end="")
print('%.3f' % score)

## 5. Naive Bayes using scikit-learn 

# Import the necessary modules
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Load the dataset
skySurvey = pd.read_csv('SDSS.csv')

# Create a new feature from u - g
skySurvey['u_g'] = skySurvey['u'] - skySurvey['g']

# Create dataframe X with features redshift and u_g
X =  skySurvey[['redshift', 'u_g']]

# Create dataframe y with feature class
y =  skySurvey[['class']]

# Initialize a Gaussian naive Bayes model
skySurveyNBModel = GaussianNB()

# Fit the model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
skySurveyNBModel.fit(X_train, np.ravel(y_train))

# Calculate the proportion of instances correctly classified
y_pred = skySurveyNBModel.predict(X_test)
score = accuracy_score(y_test, y_pred)

# Print accuracy score
print('Accuracy score is ', end="")
print('%.3f' % score)

## 6.1. Bagging using scikit-learn 

import numpy as np
import pandas as pd
from sklearn.ensemble import BaggingRegressor

df = pd.read_csv('msleep_clean.csv')

# Create a dataframe X containing the features awake, brainwt, and bodywt, in that order
X = df[['awake', 'brainwt', 'bodywt']]

# Create a dataframe y containing sleep_rem
y = df[['sleep_rem']]

# Initialize and fit bagging regressor with 30 base estimators, a random state of 10, and oob_score=True
sleepModel = BaggingRegressor(n_estimators=30, random_state=10, oob_score=True)
sleepModel.fit(X, y)

# Calculate out-of-bag accuracy
print(np.round(sleepModel.oob_score_, 4))

# Calculate predictions from out-of-bag estimate
print(np.round(sleepModel.oob_prediction_, 4))

## 6.2. Random forests using scikit-learn 

import numpy as np
import pandas as pd

from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance

df = pd.read_csv('mpg_clean.csv')

# Create a dataframe X containing the input features
X = df.drop(columns=['name', 'origin'])
# Create a dataframe y containing the output feature origin
y = df[['origin']]

# Get user-input n_estimators and max_features (ask with different values)
estimators = int(input())
max_features = int(input())

# Initialize and fit a random forest classifier with user-input number of decision trees, 
# user-input number of features considered at each split, and a random state of 123
rfModel = RandomForestClassifier(n_estimators=estimators, max_features=max_features, random_state=123)
rfModel.fit(X, y.values.ravel())

# Calculate prediction accuracy
score = rfModel.score(X, y)
print(round(score, 4))

# Calculate the permutation importance using the default parameters and a random state of 123
result = permutation_importance(rfModel, X, y, random_state=123)

# Variable importance table
importance_table = pd.DataFrame(
    data={'feature': rfModel.feature_names_in_,'permutation importance': result.importances_mean}
).sort_values('permutation importance', ascending=False)

print(importance_table)

## 6.3. Boosting using scikit-learn 

import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier

mpg = pd.read_csv('mpg.csv')

# Create a dataframe X containing cylinders, weight, and mpg
X = mpg[['cylinders', 'weight', 'mpg']]

# Create a dataframe y containing origin
y = mpg[['origin']]

# Get user-input learning rate
lr = float(input())

# Initialize and fit an adaptive boosting classifier with the user-input learning rate and a 
# random state of 123
adaBoostModel = AdaBoostClassifier(learning_rate=lr, random_state=123)
adaBoostModel.fit(X, y)

# Initialize and fit a gradient boosting classifier with the user-input learning rate and a 
# random state of 123
gradientBoostModel = GradientBoostingClassifier(learning_rate=lr, random_state=123)
gradientBoostModel.fit(X, y)

# Calculate the prediction accuracy for the adaptive boosting classifier
adaBoostScore = adaBoostModel.score(X, y)
print(round(adaBoostScore, 4))

# Calculate the prediction accuracy for the gradient boosting classifier
gradientBoostScore = gradientBoostModel.score(X, y)
print(round(gradientBoostScore, 4))
